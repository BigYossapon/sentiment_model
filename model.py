from transformers import (
    AutoModelForSequenceClassification,
    pipeline,
    AutoTokenizer,
    Trainer, 
    TrainingArguments
)
import emoji
import sys
import re
import torch
from pythainlp.spell import spell
import pandas as pd
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from pythainlp.tokenize import word_tokenize
from pythainlp.spell import correct
from pythainlp.util import emoji_to_thai, normalize
from pythainlp.corpus import thai_stopwords
from pythainlp.spell import NorvigSpellChecker
from datasets import Dataset
from huggingface_hub import get_safetensors_metadata

def preprocess_text(text):
    """ ‡πÉ‡∏ä‡πâ word_tokenize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• """
    return " ".join(word_tokenize(text, keep_whitespace=False))

def save_model(model,tokenizer,path):
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)

def load_model(path):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ
    model = AutoModelForSequenceClassification.from_pretrained(path)
    tokenizer = AutoTokenizer.from_pretrained(path)

    # device = torch.device("mps" if torch.backends.mps.is_available() else 
    #                   "cuda" if torch.cuda.is_available() else 
    #                   "cpu")
    # model.to(device)
    return model,tokenizer

def run_model(path_model):
    # Load pre-trained tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                                    'airesearch/wangchanberta-base-att-spm-uncased',
                                    revision='main')
    tokenizer.additional_special_tokens = ['<s>NOTUSED', '</s>NOTUSED', '<_>']

    # Load pre-trained model
    model = AutoModelForSequenceClassification.from_pretrained(
                                    'airesearch/wangchanberta-base-att-spm-uncased',
                                    revision='finetuned@wisesight_sentiment')
                                 
    save_model(model,tokenizer,path_model)
    
    
def run_model_from_my_hf(path_model):
        # Load pre-trained tokenizer
        model_name = "BigYossapon/SENTIMENT_TEST_FROM_WangchanBERTa"
        tokenizer = AutoTokenizer.from_pretrained(
                                        model_name,
                                        )

        # Load pre-trained model
        model = AutoModelForSequenceClassification.from_pretrained(
                                        model_name,
                                        )
                           
        save_model(model,tokenizer,path_model)

   

def train_model(path_model,path_csv):
    model,tokenizer = load_model(path=path_model)
 
    df = pd.read_csv(path_csv)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label ‡∏Å‡∏±‡∏ö text ‡πÑ‡∏´‡∏°
    if "label" not in df.columns or "text" not in df.columns:
        raise ValueError("Csv ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏±‡∏ß column ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ ‡∏´‡∏±‡∏ß‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡πÄ‡∏õ‡πá‡∏ô text ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô polarity ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô label")

    first_label = df.loc[0, "label"].strip() 
    print("Label ‡πÅ‡∏£‡∏Å:", first_label)

    label2id = {}
    is_number = False
    data = {}
    if first_label == "Positive" or first_label == "Negative" or first_label == "Neutral"  :
        label2id = {"Positive": 2, "Negative": 0, "Neutral": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "Pos" or first_label == "Neg" or first_label == "Neu"  :
        label2id = {"Pos": 2, "Neg": 0, "Neu": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "positive" or first_label == "negative" or first_label == "neutral"  :
        label2id = {"positive": 2, "negative": 0, "neutral": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "pos" or first_label == "neg" or first_label == "neu":
        label2id = {"pos": 2, "neg": 0, "neu": 1}
        df["label_id"] = df["label"].map(label2id)
    else:
        is_number = True
        
    if is_number :
        data = {
            "text": df["text"].tolist(),
            "label": df["label_id"].tolist()
        }
    else :
         data = {
        "text": df["text"].tolist(),
        "label": df["label"].tolist()
    }

    dataset = Dataset.from_dict(data)
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding="max_length", truncation=True ,max_length=512)
    # max length ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î 512 character
    dataset = Dataset.from_dict(data)
    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    training_args = TrainingArguments(
    output_dir='./results',           # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    num_train_epochs=3,              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epoch ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    per_device_train_batch_size=8,    # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    per_device_eval_batch_size=16,    # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    warmup_steps=500,                 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ warmup
    weight_decay=0.01,                # ‡∏Ñ‡πà‡∏≤ weight decay
    logging_dir='./logs',             # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö log
    logging_steps=10,                 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ log ‡∏ï‡πà‡∏≠ 10 steps
    )

    trainer = Trainer(
    model=model,                        # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    args=training_args,                 # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    train_dataset=tokenized_datasets,   # ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å
    )

    trainer.train()  

def convert_to_polarity(probabilities):
    return (
        probabilities["Neutral"] * 0.0 +   
        probabilities["Negative"] * 1.0 -  
        probabilities["Positive"] * 1.0  
    )

def split_string(text:str, chunk_size=512):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

def polarity_calculate_for_list(model,tokenizer,comment,result):
    list_string = split_string(comment)
    magnitude_avg = 0
    magnitude = 0
    for res in result :
        magnitude += res[0]["score"] 
    magnitude_avg = magnitude / len(result)
    list_neg_score = []
    list_pos_score = []
    list_neu_score = []
    list_polarity_score = []
    for string in list_string :
        inputs = tokenizer(string, return_tensors="pt", padding=True, truncation=True,max_length=512)
    # max length ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î 512 character
    #     BERT-based models (‡πÄ‡∏ä‡πà‡∏ô Camembert, BERT, etc.): ‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà 512 tokens. ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ max_length ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 512, ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î.

    # GPT-based models (‡πÄ‡∏ä‡πà‡∏ô GPT-2, GPT-3, etc.): ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏ä‡πà‡∏ô GPT-2 ‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà 1024 tokens ‡∏´‡∏£‡∏∑‡∏≠ 2048 tokens, ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏ä‡πà‡∏ô GPT-2 small, medium, large).

    # Longformer, BigBird (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏¢‡∏≤‡∏ß): ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å ‡πÜ (‡πÄ‡∏ä‡πà‡∏ô Longformer ‡∏´‡∏£‡∏∑‡∏≠ BigBird) ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 512 token (‡πÄ‡∏ä‡πà‡∏ô 4096 tokens ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô).
        with torch.no_grad():
            outputs = model(**inputs)

        # ‡πÅ‡∏õ‡∏•‡∏á logits ‡πÄ‡∏õ‡πá‡∏ô probability ‡∏î‡πâ‡∏ß‡∏¢ softmax
        probabilities = F.softmax(outputs.logits, dim=-1).squeeze().tolist()
        
        labels = ["Negative", "Neutral", "Positive"]
        prob_dict = dict(zip(labels, probabilities))

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Polarity Score
        polarity_score = convert_to_polarity(prob_dict)

        print(f"probabilities: {prob_dict}\n"  
            f"polarity_score: {polarity_score }"  )
        list_neg_score.append(prob_dict["Negative"])
        list_neu_score.append(prob_dict["Neutral"])
        list_pos_score.append(prob_dict["Positive"])

        list_polarity_score.append(polarity_score)

    score = {'Negative': sum(list_neg_score) / len(list_neg_score), 'Neutral': sum(list_neu_score) / len(list_neu_score), 'Positive': sum(list_pos_score) / len(list_pos_score)}
    average = sum(list_polarity_score) / len(list_polarity_score)
    polarity = ""
    if len(list_string)>1:
        if average > 0.5:
            polarity = "Positive"
            average = abs(average)
        elif average < -0.5:
            polarity = "Negative"
            average = -abs(average)
        else:
            average = abs(average)
            polarity = "Neutral"
    else:
        if result[0][0]["label"] == "pos":
            polarity = "Positive"
            average = abs(average)
        elif result[0][0]["label"] == "neg":
            polarity = "Negative"
            average = -abs(average)
        else: 
            polarity = "Neutral"
            average = abs(average)
    print({
            "text" : list_string,
            "magnitude" : magnitude_avg,
            "probabilities": score,  # Probability ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
            "polarity_score": average,  # ‡∏Ñ‡πà‡∏≤ Polarity Score
            "polarity" : polarity
        })
    return {
            "text" : list_string,
            "magnitude" : magnitude_avg,
            "probabilities": score,  # Probability ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
            "polarity_score": average,  # ‡∏Ñ‡πà‡∏≤ Polarity Score
            "polarity" : polarity
        }





def polarity_calculate2(model,tokenizer,comment,polarity):


    inputs = tokenizer(comment, return_tensors="pt", padding=True, truncation=True,max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)

    # ‡πÅ‡∏õ‡∏•‡∏á logits ‡πÄ‡∏õ‡πá‡∏ô probability ‡∏î‡πâ‡∏ß‡∏¢ softmax
    probabilities = F.softmax(outputs.logits, dim=-1).squeeze().tolist()
    
    # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
    labels = ["Negative", "Neutral", "Positive"]
    prob_dict = dict(zip(labels, probabilities))

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Polarity Score
    polarity_score = convert_to_polarity(prob_dict)
    # print(f"probabilities: {prob_dict}\n"  
    #     f"polarity_score: {polarity_score }"  )
    return {
        "probabilities": prob_dict,  # Probability ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
        "polarity_score": polarity_score  # ‡∏Ñ‡πà‡∏≤ Polarity Score
    }
    
def polarity_calculate_new(model,tokenizer,comment,polarity):


    inputs = tokenizer(comment, return_tensors="pt", padding=True, truncation=True,max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)

    # ‡πÅ‡∏õ‡∏•‡∏á logits ‡πÄ‡∏õ‡πá‡∏ô probability ‡∏î‡πâ‡∏ß‡∏¢ softmax
    probabilities = F.softmax(outputs.logits, dim=-1).squeeze().tolist()
    
    # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
    labels = ["Negative", "Neutral", "Positive"]
    prob_dict = dict(zip(labels, probabilities))

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Polarity Score
    polarity_score = convert_to_polarity(prob_dict)
    if polarity == "neg":
        polarity_score = -abs(polarity_score)  # Always return the negative value
    elif polarity == "pos":
        polarity_score =  abs(polarity_score)  # Always return the positive value
    

    print(
        f"polarity_score: {polarity_score }"  )
    return {
        "probabilities": prob_dict,  # Probability ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
        "polarity_score": polarity_score  # ‡∏Ñ‡πà‡∏≤ Polarity Score
    }


def use_model_for_sentiment(list_comment,path_csv,path_model):

    model , tokenizer = load_model(path_model)
    model.eval()
    classify_sequence = pipeline(task='sentiment-analysis',
            tokenizer=tokenizer,
            model=model)

    for comment in list_comment :

        processed_input_text = preprocess_text(comment)
        print('\n', processed_input_text, '\n')
        result =  classify_sequence(processed_input_text)
        
        print(result[0])
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ logits ‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        # polarity_calculate(model,tokenizer=tokenizer,comment=comment)
        # polarity_calculate2(model,tokenizer=tokenizer,comment=comment)
        polarity_calculate_for_list(model,tokenizer=tokenizer,comment=comment)
        polarity_calculate_new(model,tokenizer=tokenizer,comment=comment,polarity=result[0]['label'])
        
def use_my_model_for_sentiment_new(list_comment,path_csv,path_model):

    model , tokenizer = load_model(path_model)
    model.eval()
    classify_sequence = pipeline(task='sentiment-analysis',
            tokenizer=tokenizer,
            model=model)

    for comment in list_comment :
        list_string = split_string(comment)
        print("=====================")
        print(list_string)
        result = []
        for string in list_string:
            
            processed_input_text = preprocess_text(string)
            # print('\n', processed_input_text, '\n')
            
            result.append(classify_sequence(processed_input_text))
            # print(result)
        # print(result[0])
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ logits ‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        # polarity_calculate(model,tokenizer=tokenizer,comment=comment)
        # polarity_calculate2(model,tokenizer=tokenizer,comment=comment)
        # polarity_calculate3(model,tokenizer=tokenizer,comment=comment)
        polarity_calculate_for_list(model,tokenizer=tokenizer,comment=comment,result=result)
        # polarity_calculate_new(model,tokenizer=tokenizer,comment=comment,polarity=result[0]['label'])
    
def use_my_model_for_sentiment(list_comment,path_csv,path_model):

    model , tokenizer = load_model(path_model)
    model.eval()
    classify_sequence = pipeline(task='sentiment-analysis',
            tokenizer=tokenizer,
            model=model)
    # input_text = "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ô‡∏µ‡πâ‡∏î‡∏π‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏≠‡∏Å‡∏ï‡πà‡∏≠"
    # input_text = "‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏Å‡πÜ"
    for comment in list_comment :
        list_string = split_string(comment)
       
            
        processed_input_text = preprocess_text(comment)
        # print('\n', processed_input_text, '\n')
        result =  classify_sequence(processed_input_text)
    
        # print(result[0])
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ logits ‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        # polarity_calculate(model,tokenizer=tokenizer,comment=comment)
        # polarity_calculate2(model,tokenizer=tokenizer,comment=comment)
        # polarity_calculate3(model,tokenizer=tokenizer,comment=comment)
        polarity_calculate_for_list(model,tokenizer=tokenizer,comment=comment,magnitude=result[0]['label'])
        # polarity_calculate_new(model,tokenizer=tokenizer,comment=comment,polarity=result[0]['label'])
      
def evaluate_sentiment(model_path,csv_path):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model, tokenizer = load_model(model_path)
    classify_sequence = pipeline(task='sentiment-analysis', tokenizer=tokenizer, model=model)

    # ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV
    df = pd.read_csv(csv_path)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'text' ‡πÅ‡∏•‡∏∞ 'label'")

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞ label ‡∏à‡∏£‡∏¥‡∏á
    true_labels = []
    predicted_labels = []
    target_names = []

    # ‡πÅ‡∏õ‡∏•‡∏á label ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    first_label = df.loc[0, "label"].strip()  # .strip() ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏°‡∏≤
    print("Label ‡πÅ‡∏£‡∏Å:", first_label)
    label_map = {}
    if first_label == "Positive" or first_label == "Negative" or first_label == "Neutral"  :
        label_map = {"Positive": "pos", "Negative": "neg", "Neutral": "neu"}
        # target_names = 
    elif first_label == "Pos" or first_label == "Neg" or first_label == "Neu"  :
        label_map = {"Pos": "pos", "Neg": "neg", "Neu": "neu"}
       
    elif first_label == "positive" or first_label == "negative" or first_label == "neutral"  :
        label_map = {"positive": "pos", "negative": "neg", "neutral": "neu"}
       
    elif first_label == "pos" or first_label == "neg" or first_label == "neu":
        label_map = {"pos": "pos", "neg": "neg", "neu": "neu"}
        
    else:
        label_map = {2: "pos", 0: "neg", 1: "neu"}
        # is_number = True
    # label_map = {
    #     "pos": "pos",
    #     "neg": "neg",
    #     "neu": "neu",
    #     # "q":"q"
    # }

    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    for index, row in df.iterrows():
        comment = preprocess_text(row['text'])
        true_label = row['label']  # ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á

        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• sentiment
        result = classify_sequence(comment)
        predicted_label = result[0]['label']

        # ‡πÅ‡∏õ‡∏•‡∏á label ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ
        predicted_labels.append(label_map.get(predicted_label, "unknown"))
        true_labels.append(true_label)

        print(f"comment : {comment}")
        print(f"true value : {true_label}, üî∏ predict : {predicted_label}\n")

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    accuracy = accuracy_score(true_labels, predicted_labels)
    print(f"\n Accuracy: {accuracy:.2%}\n")
    
    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    print(classification_report(true_labels, predicted_labels, target_names=["pos", "neu", "neg"]))
  

def upload_model_to_hub(path_model,username_hf,model_name):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ
    
    model = AutoModelForSequenceClassification.from_pretrained(path_model)
    tokenizer = AutoTokenizer.from_pretrained(path_model)

    # ‡∏ä‡∏∑‡πà‡∏≠ repository ‡∏ó‡∏µ‡πà push ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hugging Face
    repo_name = f"{username_hf}/{model_name}"  # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠user‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

    # push ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
    model.push_to_hub(repo_name)
    tokenizer.push_to_hub(repo_name)

stopword_list = frozenset(thai_stopwords())

def clean_text(text):
    text = text.lower()  
    text = emoji.replace_emoji(text, replace="") 
    text = re.sub(r"[^\w\s]", "", text)  
    text = re.sub(r"(.)\1{2,}", r"\1", text)
    text = normalize(text) 
    checker = NorvigSpellChecker()
    words =  spell(text)
    corrected_words = []
    for word in words:
        if word in ["‡∏û‡πà‡∏≠‡πÅ‡∏°‡πà", "‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á"]: 
            corrected_words.append(word)
        else:
            corrected = checker.correct(word=word) 
            corrected_words.append(corrected if corrected else word)
    return " ".join(corrected_words)

def clean_text_csv(csv_path_to_clean:str):
    df = pd.read_csv(csv_path_to_clean)
    df["clean_text"] = df["text"].apply(clean_text)
    df.to_csv(csv_path_to_clean, index=False, encoding="utf-8-sig")
    
def check_memory_required(model_id = ""):
    dtype_bytes =  {"F32":4,"F16":2,"BF16":2,"FB":1}
    
    metadata = get_safetensors_metadata(model_id)
    memory = (sum(count * dtype_bytes[key.split("_")[0]] for key ,count in metadata.parameter_count.items())/(1024**3)*1.18)
    
    print(f"{model_id=} requires {memory=}GB")
    
def tokenize_function(tokenizer,examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

if __name__ == "__main__":
    list_comment_for_sentiment = []
    if len(sys.argv) > 1:
        print(sys.argv[1:])
        list_comment_for_sentiment = sys.argv[1:]
    else :
        list_comment_for_sentiment = ["‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÅ‡∏•‡∏∞‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡πÑ‡∏î‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Å‡∏≤‡∏£‡πÇ‡∏ó‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡∏≠‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏Ñ‡∏ô‡πÑ‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á ‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡πà‡∏á‡∏´‡∏±‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡πÑ‡∏Å‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏°‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÇ‡∏£‡∏Ñ‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏ö‡∏õ‡∏∞‡∏™‡∏±‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏°‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏¢‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏Ñ‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ ‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡πÉ‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡πÇ‡∏•‡∏Å ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏° ‡∏Å‡∏≤‡∏£‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Å‡πá‡∏≠‡∏≤‡∏à‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏≤‡∏£‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î‡∏™‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏™‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏±‡∏Å","‡∏ó‡∏£‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏î‡∏π‡πÅ‡∏õ‡∏•‡∏Å‡πÜ","‡∏ú‡∏°‡∏•‡∏∞‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î‡∏Ñ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ","‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏Ç‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ","‡∏£‡∏ñ‡πÑ‡∏ü‡∏â‡∏∂‡∏Å‡∏â‡∏∂‡∏Å‡∏â‡∏∂‡∏Å‡∏â‡∏±‡∏Å‡∏â‡∏±‡∏Å‡∏â‡∏±‡∏Å","‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡πÜ"]
        # python model.py "data" "delta" 
    path_model = "./model_sentiment"
    path_csv = "./datasets/data_test.csv"
    username_hf= "BigYossapon"
    model_name = "SENTIMENT_TEST_FROM_WangchanBERTa"
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    # run_model_from_my_hf("./my_model_hf")
    # use_my_model_for_sentiment_new(list_comment_for_sentiment,path_csv=path_csv,path_model="./my_model_hf")
    # evaluate_sentiment(model_path="./my_model_hf",csv_path=path_csv)
    clean_text_csv("./datasets/data_for_clean_test.csv")
    
    # 1 ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ model
    # path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö save model
    # run_model(path_model)

    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ train
    # train_model(path_model=path_model,path_csv=path_csv)

    # 2 ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ   
    # use_model_for_sentiment(list_comment_for_sentiment,path_csv=path_csv,path_model=path_model)

    # 3 ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏™ accuracy
    # evaluate_sentiment(model_path=path_model,csv_path=path_csv)

    #‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏á‡πâ‡∏ó‡∏µ‡πà hunging face 1.‡∏™‡∏°‡∏±‡∏Ñ‡∏£ 2.huggingface-cli login 3.‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á model ‡πÑ‡∏ß‡πâ 
    # upload_model_to_hub(path_model=path_model,model_name=model_name,username_hf=username_hf)
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ eng ‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤ neutral ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô eng ‡πÄ‡∏•‡∏¢

    # ‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô datasets ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏±‡∏ß column ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ column ‡πÄ‡∏õ‡πá‡∏ô text ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤ polarity ‡πÄ‡∏õ‡πá‡∏ô label 
