from transformers import (
    CamembertTokenizer,
    AutoModelForSequenceClassification,
    TFAutoModelForSequenceClassification,
    pipeline,
    AutoTokenizer,
    Trainer, 
    TrainingArguments
)
import os
import sys
import torch
import pandas as pd
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report



# from transformers import TFAutoModelForSequenceClassification
from pythainlp.tokenize import word_tokenize
# from thai2transformers.preprocess import process_transformers
from datasets import Dataset



def preprocess_text(text):
    """ ‡πÉ‡∏ä‡πâ word_tokenize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• """
    return " ".join(word_tokenize(text, keep_whitespace=False))

def save_model(model,tokenizer,path):
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    # save_directory = './my_model'

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
    model.save_pretrained(path)
    tokenizer.save_pretrained(path)

def load_model(path):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ
    model = AutoModelForSequenceClassification.from_pretrained(path)
    tokenizer = AutoTokenizer.from_pretrained(path)

    device = torch.device("mps" if torch.backends.mps.is_available() else 
                      "cuda" if torch.cuda.is_available() else 
                      "cpu")
    model.to(device)
    return model,tokenizer

def run_model(path_model):
    # Load pre-trained tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                                    'airesearch/wangchanberta-base-att-spm-uncased',
                                    revision='main')
    tokenizer.additional_special_tokens = ['<s>NOTUSED', '</s>NOTUSED', '<_>']

    # Load pre-trained model
    model = AutoModelForSequenceClassification.from_pretrained(
                                    'airesearch/wangchanberta-base-att-spm-uncased',
                                    revision='finetuned@wisesight_sentiment')

    # model = TFAutoModelForSequenceClassification.from_pretrained(
    #                                 'airesearch/wangchanberta-base-att-spm-uncased',
    #                                 revision='finetuned@wisesight_sentiment')                                
    save_model(model,tokenizer,path_model)

   

def train_model(path_model,path_csv):
    model,tokenizer = load_model(path=path_model)
 
    df = pd.read_csv(path_csv)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå label ‡∏Å‡∏±‡∏ö text ‡πÑ‡∏´‡∏°
    if "label" not in df.columns or "text" not in df.columns:
        raise ValueError("CSV file must contain 'label' and 'text' columns")

    # (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á label ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
    first_label = df.loc[0, "label"].strip()  # .strip() ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏°‡∏≤
    print("Label ‡πÅ‡∏£‡∏Å:", first_label)

    label2id = {}
    is_number = False
    data = {}
    if first_label == "Positive" or first_label == "Negative" or first_label == "Neutral"  :
        label2id = {"Positive": 2, "Negative": 0, "Neutral": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "Pos" or first_label == "Neg" or first_label == "Neu"  :
        label2id = {"Pos": 2, "Neg": 0, "Neu": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "positive" or first_label == "negative" or first_label == "neutral"  :
        label2id = {"positive": 2, "negative": 0, "neutral": 1}
        df["label_id"] = df["label"].map(label2id)
    elif first_label == "pos" or first_label == "neg" or first_label == "neu":
        label2id = {"pos": 2, "neg": 0, "neu": 1}
        df["label_id"] = df["label"].map(label2id)
    else:
        is_number = True
        
    


    # label2id = {"Positive": 2, "Negative": 0, "Neutral": 1}
    # df["label_id"] = df["label"].map(label2id)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô
    if is_number :
        data = {
            "text": df["text"].tolist(),
            "label": df["label_id"].tolist()
        }
    else :
         data = {
        "text": df["text"].tolist(),
        "label": df["label"].tolist()
    }

    dataset = Dataset.from_dict(data)
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding="max_length", truncation=True ,max_length=128)

    dataset = Dataset.from_dict(data)
    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    training_args = TrainingArguments(
    output_dir='./results',           # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    num_train_epochs=3,              # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epoch ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    per_device_train_batch_size=8,    # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    per_device_eval_batch_size=16,    # ‡∏Ç‡∏ô‡∏≤‡∏î batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    warmup_steps=500,                 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ warmup
    weight_decay=0.01,                # ‡∏Ñ‡πà‡∏≤ weight decay
    logging_dir='./logs',             # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö log
    logging_steps=10,                 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ log ‡∏ï‡πà‡∏≠ 10 steps
    )

    trainer = Trainer(
    model=model,                        # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    args=training_args,                 # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    train_dataset=tokenized_datasets,   # ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å
    )

    trainer.train()  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•

def convert_to_polarity(probabilities):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ Probability ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Sentiment Classification ‡πÄ‡∏õ‡πá‡∏ô Polarity Score
    """
    return (
        probabilities["Positive"] * 1.0 +  # Positive ‡πÄ‡∏õ‡πá‡∏ô 1.0
        probabilities["Neutral"] * 0.0 +   # Neutral ‡πÄ‡∏õ‡πá‡∏ô 0.0
        probabilities["Negative"] * -1.0   # Negative ‡πÄ‡∏õ‡πá‡∏ô -1.0
    )

def polarity_calculate2(model,tokenizer,comment):
   
   
    inputs = tokenizer(comment, return_tensors="pt", padding=True, truncation=True,max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)

    # ‡πÅ‡∏õ‡∏•‡∏á logits ‡πÄ‡∏õ‡πá‡∏ô probability ‡∏î‡πâ‡∏ß‡∏¢ softmax
    probabilities = F.softmax(outputs.logits, dim=-1).squeeze().tolist()
    
    # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
    labels = ["Negative", "Neutral", "Positive"]
    prob_dict = dict(zip(labels, probabilities))

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Polarity Score
    polarity_score = convert_to_polarity(prob_dict)
    print(f"probabilities: {prob_dict}\n"  
        f"polarity_score: {polarity_score }"  )
    return {
        "probabilities": prob_dict,  # Probability ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ class
        "polarity_score": polarity_score  # ‡∏Ñ‡πà‡∏≤ Polarity Score
    }


def use_model_for_sentiment(list_comment,path_csv,path_model):

    model , tokenizer = load_model(path_model)
    model.eval()
    classify_sequence = pipeline(task='sentiment-analysis',
            tokenizer=tokenizer,
            model=model)
    # input_text = "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ô‡∏µ‡πâ‡∏î‡∏π‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏≠‡∏Å‡∏ï‡πà‡∏≠"
    # input_text = "‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏Å‡πÜ"
    for comment in list_comment :

        processed_input_text = preprocess_text(comment)
        print('\n', processed_input_text, '\n')
        print(classify_sequence(processed_input_text))
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ logits ‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        polarity_calculate(model,tokenizer=tokenizer,comment=comment)
        polarity_calculate2(model,tokenizer=tokenizer,comment=comment)
    
        
      
  

def polarity_calculate(model,tokenizer,comment):
   
         # Tokenize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, padding=True)
    
    # ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    outputs = model(**inputs)
    logits = outputs.logits
    
    # ‡πÉ‡∏ä‡πâ softmax ‡∏Å‡∏±‡∏ö logits ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô probabilities
    probs = F.softmax(logits, dim=-1)
    
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏°‡∏µ 3 ‡∏Ñ‡∏•‡∏≤‡∏™ (negative, neutral, positive)
    negative_prob = probs[0, 0].item()
    neutral_prob = probs[0, 1].item()
    positive_prob = probs[0, 2].item()
    question_prob = probs[0, 3].item()
        
        # # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Polarity Score ‡πÅ‡∏ö‡∏ö Weighted Average
        # polarity_score = (negative_prob - positive_prob) / (positive_prob + neutral_prob + negative_prob)
        # print(f"polarity score : {polarity_score}")
        
        # return polarity_score

   
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Polarity Score ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á positive ‡πÅ‡∏•‡∏∞ negative
    polarity_score_1 = (positive_prob - negative_prob)
    print(f"Polarity Score (positive - negative): {polarity_score_1}")

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Polarity Score ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ weighted average ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    polarity_score_2 = (negative_prob - positive_prob) / (positive_prob + neutral_prob + negative_prob + question_prob)
    print(f"Polarity Score (Weighted Average): {polarity_score_2}")

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
    polarity_score_3 = (positive_prob + neutral_prob - negative_prob)
    print(f"Polarity Score (Summed): {polarity_score_3}")

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 4: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Polarity Score ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏ß‡∏Å‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 0-1
    polarity_score_4 = (positive_prob - negative_prob) / (positive_prob + negative_prob)
    print(f"Polarity Score (Normalized): {polarity_score_4}")

    # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    print(f"Polarity Score (positive - negative): {polarity_score_1}, "
          f"Polarity Score (Weighted Average): {polarity_score_2}, "
          f"Polarity Score (Summed): {polarity_score_3}, "
          f"Polarity Score (Normalized): {polarity_score_4}")

def evaluate_sentiment(csv_path):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model, tokenizer = load_model("./model_sentiment")
    classify_sequence = pipeline(task='sentiment-analysis', tokenizer=tokenizer, model=model)

    # ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV
    df = pd.read_csv(csv_path)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'text' ‡πÅ‡∏•‡∏∞ 'label'")

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞ label ‡∏à‡∏£‡∏¥‡∏á
    true_labels = []
    predicted_labels = []
    target_names = []

    # ‡πÅ‡∏õ‡∏•‡∏á label ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    first_label = df.loc[0, "label"].strip()  # .strip() ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏°‡∏≤
    print("Label ‡πÅ‡∏£‡∏Å:", first_label)
    label_map = {}
    if first_label == "Positive" or first_label == "Negative" or first_label == "Neutral"  :
        label_map = {"Positive": "pos", "Negative": "neg", "Neutral": "neu"}
        # target_names = 
    elif first_label == "Pos" or first_label == "Neg" or first_label == "Neu"  :
        label_map = {"Pos": "pos", "Neg": "neg", "Neu": "neu"}
       
    elif first_label == "positive" or first_label == "negative" or first_label == "neutral"  :
        label_map = {"positive": "pos", "negative": "neg", "neutral": "neu"}
       
    elif first_label == "pos" or first_label == "neg" or first_label == "neu":
        label_map = {"pos": "pos", "neg": "neg", "neu": "neu"}
        
    else:
        label_map = {2: "pos", 0: "neg", 1: "neu"}
        # is_number = True
    # label_map = {
    #     "pos": "pos",
    #     "neg": "neg",
    #     "neu": "neu",
    #     # "q":"q"
    # }

    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    for index, row in df.iterrows():
        comment = preprocess_text(row['text'])
        true_label = row['label']  # ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á

        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• sentiment
        result = classify_sequence(comment)
        predicted_label = result[0]['label']

        # ‡πÅ‡∏õ‡∏•‡∏á label ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ
        predicted_labels.append(label_map.get(predicted_label, "unknown"))
        true_labels.append(true_label)

        print(f"‚úÖ ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå: {comment}")
        print(f"üîπ ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á: {true_label}, üî∏ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {predicted_label}\n")

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    accuracy = accuracy_score(true_labels, predicted_labels)
    print(f"\nüìä Accuracy: {accuracy:.2%}\n")
    
    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    print(classification_report(true_labels, predicted_labels, target_names=["pos", "neu", "neg"]))
  

def upload_model_to_hub(path_model,username_hf,model_name):
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ
    
    model = AutoModelForSequenceClassification.from_pretrained(path_model)
    tokenizer = AutoTokenizer.from_pretrained(path_model)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ repository ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ push ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Hugging Face
    repo_name = f"{username_hf}/{model_name}"  # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

    # push ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
    model.push_to_hub(repo_name)
    tokenizer.push_to_hub(repo_name)


def tokenize_function(tokenizer,examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

if __name__ == "__main__":
    list_comment_for_sentiment = []
    if len(sys.argv) > 1:
        print(sys.argv[1:])
        list_comment_for_sentiment = sys.argv[1:]
    else :
        list_comment_for_sentiment = ["‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÉ‡∏à‡∏î‡∏µ ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏î‡∏π‡∏°‡∏µ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï","‡∏ó‡∏£‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏î‡∏π‡πÅ‡∏õ‡∏•‡∏Å‡πÜ","‡∏ú‡∏°‡∏•‡∏∞‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î‡∏Ñ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ","‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏Ç‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ"]
        # python model.py "data" "delta" 
    path_model = "./model_sentiment"
    path_csv = "./datasets/twitter_training.csv"
    username_hf= "BigYossapon"
    model_name = "SENTIMENT_TEST_FROM_WangchanBERTa"

    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ model
    # run_model(path_model)

    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ train
    # train_model(path_model=path_model,path_csv=path_csv)

    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ   
   
    use_model_for_sentiment(list_comment_for_sentiment,path_csv=path_csv,path_model=path_model)

    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏™ accuracy
    # evaluate_sentiment(path_csv)

    #‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏á‡πâ‡∏ó‡∏µ‡πà hunging face 1.‡∏™‡∏°‡∏±‡∏Ñ‡∏£ 2.huggingface-cli login 3.‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á model ‡πÑ‡∏ß‡πâ 
    # upload_model_to_hub(path_model=path_model,model_name=model_name,username_hf=username_hf)
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ eng ‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡∏ß‡πà‡∏≤ neutral ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô eng ‡πÄ‡∏•‡∏¢

